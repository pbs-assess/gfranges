---
title: "R Notebook"
output: html_notebook
---

```{r setup}
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) 
library(gfplot)
library(lattice)
library(INLA)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(mapdata)
library(lubridate)
```

```{r}
ssids <- get_ssids()
ssids
```

```{r}
raw <- readRDS("data-cache/pacific-cod.rds")
#dat$survey_abbrev
glimpse(raw)
```

```{r}
testdat <- tidy_survey_sets(raw, c("SYN HS","SYN QCS","SYN WCVI","SYN WCHG"), years = c(2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018))
```


Get sensor data
```{r}
# get_ssids()
# sensor_syn <- get_sensor_data_trawl(ssid = c(1, 3, 4, 16), spread_attributes = TRUE)
# View(sensor_syn)
# saveRDS(sensor_syn, "data-cache/allsynoptic_sensor_07mar19_time2.rds")
# write.csv(sensor, "synoptic_3_sensor_data_07mar19.csv")
# sensor <- read_csv("data-cache/synoptic_3_sensor_data_07mar19.csv", col_types = cols(do_mlpL_avg = col_double(), 
#                     do_mlpL_max = col_double(), do_mlpL_min = col_double(), salinity_PSU_avg = col_double(), salinity_PSU_max = col_double(), 
#                     salinity_PSU_min = col_double()), na = "NA")
sensor <- readRDS("data-cache/allsynoptic_sensor_07mar19_time2.rds")
glimpse(sensor)
# View(sensor)
```


Join sensor data with trawl data from tidy_survey_sets function
```{r}
dat <- left_join(testdat, sensor, by="fishing_event_id", suffix = c("", ".sensor"))
dat$year_f <- factor(dat$year)
# View(dat)
dat <- dat %>% mutate(do_range = do_mlpL_max - do_mlpL_min, salinity_range = salinity_PSU_max -
  salinity_PSU_min, temp_range = temperature_C_max - temperature_C_min, depth_range = depth_m_max - depth_m_min)
# glimpse(dat)
```


Optional: Interpolate missing depth data
```{r, eval=FALSE}
testdat2 <- gfplot:::interp_survey_bathymetry(testdat)
glimpse(testdat2[[1]])
# if bathymetry ran...
dat <- left_join(testdat2[[1]], sensor, by="fishing_event_id", suffix = c("", ".sensor"))
dat$year_f <- factor(dat$year)
# View(dat)
dat <- dat %>% mutate(do_range = do_mlpL_max - do_mlpL_min, salinity_range = salinity_PSU_max -
  salinity_PSU_min, temp_range = temperature_C_max - temperature_C_min, depth_range = depth_m_max - depth_m_min)
```



```{r}
plot(depth ~ depth_m_avg, data = dat) # both depth variables seem to agree

plot(temp_range ~ depth_range, data = dat) 
plot(do_range ~ depth_range, data = dat) 
plot(salinity_range ~ depth_range, data = dat) 

plot(temperature_C_avg ~ temp_range, data = dat) #variance in temp estimate is highest for middle values
plot(do_mlpL_avg ~ do_range, data = dat) # one outlier at range = 8, others < 4
plot(do_range ~ depth, data = dat) # but outlier is at middle depth
plot(salinity_PSU_avg ~ salinity_range, data = dat) # 5 extreme values range > 10 and abnormally low mean salinity 
plot(salinity_PSU_avg ~ depth, cex = salinity_range/3, ylim=c(25,35), data = dat)
plot(salinity_PSU_max ~ depth, cex = salinity_range/3, ylim=c(25,35), data = dat)
plot(do_mlpL_avg ~ depth, cex = do_range, data = dat) 
plot(temperature_C_avg ~ depth, cex = temp_range, data = dat)

plot(do_mlpL_avg ~ temperature_C_avg, cex = depth/100, data = dat)
plot(do_mlpL_avg ~ salinity_PSU_avg, cex = depth/100, data = dat)

plot(-log(depth) ~ do_mlpL_avg,  cex = depth_range/35, data = dat) 
plot(-log(depth) ~ temperature_C_avg,  cex = depth_range/35, data = dat)
plot(-log(depth) ~ salinity_PSU_max,  cex = depth_range/35, data = dat)
```

```{r}
dat2 <- dat %>% filter(temperature_C_avg!="NA") %>% filter(ssid==1)

ggplot(dat2, aes(x=depth, y=temperature_C_avg)) + 
  geom_point(alpha=0.25) + 
  geom_point(data=subset(dat2, temp_range>1), alpha=0.5) + 
  geom_errorbar(data=subset(dat2, temp_range>1),aes(ymin=temperature_C_min, ymax=temperature_C_max),alpha= 0.75, colour="blue") + 
  geom_errorbarh(data=subset(dat2, temp_range>1),aes(xmin=depth_m_min, xmax=depth_m_max),alpha= 0.75, colour="red") + 
  ylim(2,12) + 
  facet_grid(rows = vars(year), cols = vars(ssid), scales = "free")
```

```{r}
dat2 <- dat %>% filter(do_mlpL_avg!="NA")

ggplot(dat2, aes(x=depth, y=do_mlpL_avg)) + 
  geom_point(alpha=0.25) + 
  geom_point(data=subset(dat2, do_range>1), alpha=0.5) + 
  geom_errorbar(data=subset(dat2, do_range>1), aes(ymin=do_mlpL_min, ymax=do_mlpL_max), alpha= 0.75, colour="blue") + 
  geom_errorbarh(data=subset(dat2, do_range>1), aes(xmin=depth_m_min, xmax=depth_m_max), alpha= 0.75, colour="red") + 
  ylim(0,10) + 
  facet_wrap(~survey_desc, scales = "free")
```

```{r}
dat2 <- dat %>% filter(salinity_PSU_avg!="NA")

ggplot(dat2, aes(x=depth, y=salinity_PSU_avg)) + 
  geom_point(alpha=0.25) + 
  geom_point(data=subset(dat2, salinity_range>1), alpha=0.5) + 
  geom_errorbar(data=subset(dat2, salinity_range>1), aes(ymin=salinity_PSU_min, ymax=salinity_PSU_max), alpha= 0.75, colour="blue") +
  geom_errorbarh(data=subset(dat2, salinity_range>1), aes(xmin=depth_m_min, xmax=depth_m_max), alpha= 0.75, colour="red") + 
  #ylim(0,10) + 
  facet_wrap(~survey_desc, scales = "free")
```


```{r}
out <- dat %>% filter(depth_range>80)
out

pair <- dat %>% filter(ssid=="1") %>% select(., depth_range, temp_range, do_range, salinity_range, depth_m_avg,do_mlpL_avg,temperature_C_avg,salinity_PSU_max)
pairs(pair)  
```

Scale predictors by modifying code from gfplot to include environmental variables...
Changes should be added to gfplot?
```{r}
scale_survey_predictors <- function(dat) {
  if (sum(is.na(dat$depth)) > 0) {
    dat$depth[is.na(dat$depth)] <- dat$akima_depth[is.na(dat$depth)]
  }
  mutate(dat,
    # depth
    depth_mean = mean(log(depth), na.rm = TRUE),
    depth_c = (log(depth) - depth_mean[1]),
    depth_sd = sd(log(depth), na.rm = TRUE),
    depth_scaled = (log(depth) - depth_mean[1]) / depth_sd[1],
    depth_scaled2 = depth_scaled^2,
    # temperature
    temp_mean = mean(log(temperature_C_avg), na.rm = TRUE),
    temp_c = (log(temperature_C_avg) - temp_mean[1]),
    temp_sd = sd(log(temperature_C_avg), na.rm = TRUE),
    temp_scaled = (log(temperature_C_avg) - temp_mean[1]) / temp_sd[1],
    temp_scaled2 = temp_scaled^2,
    # dissolved O2
    do_mean = mean(log(do_mlpL_avg), na.rm = TRUE),
    do_c = (log(do_mlpL_avg) - do_mean[1]),
    do_sd = sd(log(do_mlpL_avg), na.rm = TRUE),
    do_scaled = (log(do_mlpL_avg) - do_mean[1]) / do_sd[1],
    do_scaled2 = do_scaled^2,
    # salinity
    salinity_mean = mean(log(salinity_PSU_avg), na.rm = TRUE),
    salinity_c = (log(salinity_PSU_avg) - salinity_mean[1]),
    salinity_sd = sd(log(salinity_PSU_avg), na.rm = TRUE),
    salinity_scaled = (log(salinity_PSU_avg) - salinity_mean[1]) / salinity_sd[1],
    salinity_scaled2 = salinity_scaled^2,
    # to put spatial decay parameter on right scale
    X10 = X / 10, Y10 = Y / 10 
  )
}
```

```{r}
scaled <- dat %>% filter(depth != "NA") %>% scale_survey_predictors()
# glimpse(scaled)
temp_dat <- scaled %>% filter (density!="NA") %>% filter ( ssid =="1") %>% filter(temperature_C_avg!="NA") # %>% filter(year==2017)
# glimpse(temp_dat)
# do_dat <- scaled %>% filter (density!="NA") %>% filter ( ssid =="1") %>%filter(do_mlpL_avg!="NA")
#pg <- readRDS("prediction-grids/QCSpred-grid-interp-cell-width-2.rds")
#pg <- as.array(pg)

```


```{r}
ggplot(temp_dat, aes(depth)) + geom_histogram() 
ggplot(temp_dat, aes(density*1000)) + geom_histogram() 
ggplot(temp_dat, aes(depth, (density*1000))) + geom_point() 
#ggplot(synhs, aes(depth_m, log(density*1000+1))) + geom_point() 
```

######################################
 Start Bayesian analysis
 One more time the model:
 Movement_ij      ~ Gamma(mu_ik, r)
 E(Movement_ij)   = mu_ij
 var(Movement_ij) = mu_ij^2 / r
 mu_ij = exp(Intercept + fRepro + Annual Trend + Seasonal Trend + a_i)
 Below we define a model with a factor fRepro, a long term
 trend for Year, and seasonal trend for DayInYear, and
 a random intercept for bear.
 Each trend is modelled as a random walk trend.
 So each of them has a sigma.


```{r}

d <- temp_dat
response = "temperature_C_avg"
n_knots = 50
family = "gaussian"
plot = TRUE
fit_model = TRUE
extend = list(n = 8, offset = -0.1)
RangeGuess = 100 
offset = c(RangeGuess/5, RangeGuess)
max_edge = c(RangeGuess/5,RangeGuess)
cutoff = (RangeGuess/5)/5
prior_range = c(RangeGuess, 0.01) # P(range < 0.05) = 0.01
prior_sigma = c(0.5, 0.5)
kmeans = FALSE    
verbose = FALSE
debug = FALSE
trials = 2
covariates = c("depth_scaled", "depth_scaled2")
time = "year"
sp_model_type = "ar1"
formula = "full_model"

fit_inla_sensor <- function(d, response = "temperature_C_avg",
                     n_knots = 50,
                     family = "binomial",
                     plot = FALSE,
                     fit_model = TRUE,
                     sp_model_type = "ar1",
                     extend = list(n = 8, offset = -0.1),
                     kmeans = FALSE,                   
                     RangeGuess = 100, 
                     offset = c(RangeGuess/5, RangeGuess),                 
                     max_edge = c(RangeGuess/5,RangeGuess),
                     cutoff = (RangeGuess/5)/5,
                     prior_range = c(RangeGuess, 0.01), # P(range < 0.05) = 0.01
                     prior_sigma = c(0.5, 0.5),
                     time = "year",
                     covariates = NULL,
                     verbose = FALSE,
                     debug = FALSE,
                     trials = 10,
                     formula = full_model) {
  
  # create data.frame including response ("y"), coordinates("xcoo", "ycoo"), covariates, but not "intercept" as that will be input as time 1
  dat <- data.frame(
    y = d[, response, drop = TRUE],
    xcoo = d$X,
    ycoo = d$Y, 
    raw_time = d[, time, drop = TRUE],
    select(d, covariates)
  )
  
  # make sure dat is ordered temporally
  YO  <- order(dat$raw_time)
  dat <- dat[YO,]
  
  # number years sequentially, removing placeholders for missing years
  time_k <- as.numeric(as.factor(dat$raw_time))
  k <- max(time_k)
  
  # make design matrix with first time as intercept
  time_lab <- paste0("Y", seq(1, k)) # make vector of each year labeled "Y1", "Y2", ..."
  dat[time_lab] <- 0 # fill empty matrix with 0
  dat[, time_lab[1]] <- 1 # first time point assigned 1 for intercept
  for (j in seq_along(time_lab)) {
    dat[time_k == j, time_lab[j]] <- 1
  }
  
  ## Generate spatial mesh
  coords <- as.matrix(unique(d[, c("X", "Y")]))

  # use kmeans() to calculate centers:
  if (kmeans) {
    km <- stats::kmeans(x = coords, centers = n_knots)
    mesh2d <- INLA::inla.mesh.create(km$centers,
      extend = extend
    )
  } else {
    bnd <- INLA::inla.nonconvex.hull(coords)
    
    mesh2d <- INLA::inla.mesh.2d(
      offset = offset,
      boundary = bnd,
      max.edge = max_edge,
      cutoff = cutoff
    )
  }
  message("INLA max_edge = ", "c(", max_edge[[1]], ", ", max_edge[[2]], "), n = ", mesh2d$n)
  
  
  
  # create an object for a Matérn model
  spde <- INLA::inla.spde2.matern(mesh = mesh2d, alpha = 3 / 2) #, # WHAT DOES ALPHA DO?????
  #                                prior.range = prior_range, 
  #                                prior.sigma = prior_sigma) 
  
  ## build the space time indices
  # index = inla.spde.make.index("space", n.spde = spde$n.spde, n.group = mesh1$m)
  # construct index for ar1 model 
  iset <- INLA::inla.spde.make.index(
    name = "spatial.field",
    n.spde = spde$n.spde, 
    n.group = k
  )
  
  ## Generate depth mesh
  d_knots = seq(min(d$depth_scaled), max(d$depth_scaled), length.out = 30)
  #mesh0 = inla.mesh.1d(loc = d_knots, degree = 0, boundary = c('neumann', 'free'))
  mesh1 = inla.mesh.1d(loc = d_knots, degree = 1, boundary = c('neumann', 'free'))
  #mesh2 = inla.mesh.1d(loc = d_knots, degree = 2, boundary = c('neumann', 'free'))
    # graphics::plot(mesh0$loc,mesh0$mid)
    # graphics::points(mesh1$loc,mesh1$mid)
    # graphics::points(mesh2$loc,mesh2$mid, col = "red")
  
  idepth <- INLA::inla.spde.make.index("depth.field", n.spde = mesh1$m, n.group = k)

  if (plot) {
    graphics::plot(mesh2d, asp = 1)
    graphics::points(coords, col = "red")
  }
  
  # isolate list of covariates:
  X.1              <- dat[, -c(1:4), drop = FALSE] # remove "y", "xcoo", "ycoo", "real_time"
  covar_names      <- colnames(X.1) # list covariates
  XX.list          <- as.list(X.1) # turn data.frame to a list
  effect.list      <- list()
  effect.list[[1]] <- c(iset)
  effect.list[[2]] <- c(idepth)
  
 for (Z in seq_len(ncol(X.1)))
    effect.list[[Z + 2]] <- XX.list[[Z]] 
    
    
  names(effect.list) <- c("1", "d_knots", covar_names)
  
 # projection matrix to map to observed points
 A <- INLA::inla.spde.make.A(
    mesh = mesh2d,
    loc = cbind(dat$xcoo, dat$ycoo),
    group = time_k
  )
 A2 <- INLA::inla.spde.make.A(
    mesh = mesh1,
    loc = dat$depth_scaled,
    group = time_k
  )
  A.list <- list()
  A.list[[1]] <- A
  A.list[[2]] <- A2
  
  for (Z in seq_len(ncol(X.1)))
    A.list[[Z + 2]] <- 1
  
  # make projection points stack:
  sdat <- INLA::inla.stack(tag = "stdata",
                           data = list(y = dat$y),
                           A = A.list,
                           effects = effect.list
  )

   full_model <- as.formula(paste(
    'y ~ -1 + ',
    paste(covar_names, collapse = '+'),
    '+f(depth_scaled, time_k, model = "rw1", scale.model = TRUE) + f(spatial.field, model = spde, group = spatial.field.group, control.group = list(model ="', sp_model_type,'"))'))
  
 
  
  if (fit_model) {
    try_fitting <- TRUE
    trial_i <- 1
    while (try_fitting && trial_i <= trials) {
      model <- try(INLA::inla(formula,
                   family = family,
                   data = INLA::inla.stack.data(sdat),
                   control.predictor = list(compute = TRUE,
                                            A = INLA::inla.stack.A(sdat)),
                   control.fixed     = list(mean = 0, 
                                            prec = 1 / (5^2),
                                            mean.intercept = 0, 
                                            prec.intercept = 1 / (20^2)),
                   control.compute = list(config = TRUE),
                   verbose = verbose,
                   debug = debug,
                   keep = FALSE
                 )
      )
      trial_i <- trial_i + 1
      if (class(model) != "try-error") {
        try_fitting <- FALSE
      } else {
        # if (trial_i > 5) browser()
        message("INLA error. Trying again.")
      }
    }
    if (class(model) == "try-error") {
      model <- NA
    }
  } else {
    model <- NA
  }
#  obj <- 
    list(model = model, mesh = mesh2d, spde = spde, data = dat,
    formula = formula, iset = iset
  )
}
```




```{r}
obj <- fit_inla_sensor(temp_dat, response = "temperature_C_avg",
                       n_knots = 50,
                       family = "gaussian",
                       plot = TRUE,
                       fit_model = TRUE,
                       re_model_type = "rw1",
                       extend = list(n = 8, offset = -0.1),
                       RangeGuess = 100, 
                       covariates = c("depth_scaled", "depth_scaled2"),
                       verbose = FALSE,
                       debug = FALSE,
                       trials = 2
                       )

pg <- gfplot:::make_prediction_grid(temp_dat, survey ="SYN QCS")$grid #filter(temp_dat, year %in% c(2016, 2017))
cell_area <- gfplot:::make_prediction_grid(filter(temp_dat, year %in% c(2016, 2017)), survey = "SYN QCS")$cell_area
```



Project onto grid:

```{r , eval=TRUE, include=FALSE}
# alternative to function, hard code
# samples = 10L
# pred_grid <- pg_one_year

predict_inla_sens <- function(obj, pred_grid, 
                                   samples = 100L) {
  mesh <- obj$mesh
  model <- obj$model
  iset <- obj$iset

  inla.mcmc <- INLA::inla.posterior.sample(n = samples, model)
  
  # get indices of various effects:
  latent_names   <- rownames(inla.mcmc[[1]]$latent)
  yr_fe_indx     <- grep("^Y[0-9]+$", latent_names)
  re_indx        <- grep("^spatial.field", latent_names)
  depth_fe_indx1 <- grep("^depth_scaled$", latent_names)
  depth_fe_indx2 <- grep("^depth_scaled2$", latent_names)
  #intercept_indx <- grep("^intercept$", latent_names)

  # read in locations and knots, from projection matrix
  grid_locs      <- pred_grid[, c("X", "Y")]
  
  # define empty arrays to store projections:
  latent_grid   <- array(0, dim = c(nrow(grid_locs), samples, length(yr_fe_indx)))
  spatial_field <- array(0, dim = c(nrow(grid_locs), samples, length(yr_fe_indx)))
  proj_matrix   <- INLA::inla.spde.make.A(mesh2d, loc = as.matrix(grid_locs))
  
  # loop over all years; do MCMC projections for that year
  for (yr in seq_along(yr_fe_indx)) {
   indx <- which(iset$spatial.field.group == 1) 
      for (i in seq_len(samples)) {
      if (yr > 1) {
        year_diff <- inla.mcmc[[i]]$latent[yr_fe_indx[[yr]]]
      } else {
        year_diff <- 0
      }
        
      # constant covariate effects across years (currently hard coded...)
      fixed_covariate_effects <- 
        pred_grid[, "depth_scaled", drop = TRUE] * inla.mcmc[[i]]$latent[depth_fe_indx1] +
        pred_grid[, "depth_scaled2", drop = TRUE] * inla.mcmc[[i]]$latent[depth_fe_indx2]
   
      # spatial temporal 
      spatial_field[, i, yr] <-
         as.numeric(proj_matrix %*% inla.mcmc[[i]]$latent[re_indx][indx]) +
         #inla.mcmc[[i]]$latent[intercept_indx] + 
         inla.mcmc[[i]]$latent[yr_fe_indx[[1]]] + # base year
         year_diff # 0 for the base year, diff otherwise
      
      # model predictions (current covariates are hard coded)   
      latent_grid[, i, yr] <- spatial_field[, i, yr] + fixed_covariate_effects
   }
  }
  list(latent_grid = latent_grid, depth_effects = depth_effects, spatial_field = spatial_field)
}

```


```{r posterior-draws}
#pg_one_year <- filter(pg, year == min(pg$year))
projected <- predict_inla_sens(obj = obj, pred_grid = pg, samples = 10)
```


Plot values on the grid:

```{r prediction-grid-calcs}
attributes(projected)
pg_one_year <- filter(pg, year == min(pg$year))
attributes(projected$latent_grid)
out_med <- apply(projected$latent_grid, c(1,3), median)
pg_all <- bind_rows(replicate(dim(projected$latent_grid)[3], pg, simplify = FALSE))
out_long <- reshape2::melt(out_med)
pg_all$time <- out_long$Var2
pg_all$pred <- out_long$value
```


```{r }
attributes(projected$spatial_field)
out_med_sp <- apply(projected$spatial_field, c(1,3), median)
out_long_sp <- reshape2::melt(out_med_sp)
pg_all$sp_field <- out_long_sp$value
```


Spatiotemporal plot:

```{r spatio-temporal-plot, fig.width=9, fig.height=9}
coast <- gfplot:::load_coastline(range(d$lon), range(d$lat), utm_zone = 9)
isobath <- gfplot:::load_isobath(range(d$lon), range(d$lat),
  bath = c(100, 200, 500), utm_zone = 9
)
map_padding <- c(-5, 5)

make_plot <- function(dat, fill = "sqrt(pred)") {
  g <- ggplot(dat, aes(X * 10, Y * 10)) +
    geom_raster(aes_string(fill = fill)) +
    scale_fill_viridis_c(option = "C") +
    geom_polygon(
      data = coast, aes_string(x = "X", y = "Y", group = "PID"),
      fill = "grey80"
    ) +
    geom_point(
      data = filter(d, present == 1),
      aes(x = X * 10, y = Y * 10, size = density), inherit.aes = FALSE,
      pch = 21, col = "white", alpha = 0.3
    ) +
    geom_point(
      data = filter(d, present == 0),
      aes(x = X * 10, y = Y * 10), inherit.aes = FALSE,
      pch = 4, col = "white", alpha = 0.2
    )
  
  g <- g + geom_path(
    data = isobath, aes_string(
      x = "X", y = "Y",
      group = "paste(PID, SID)"
    ),
    inherit.aes = FALSE, lwd = 0.4, col = "grey70", alpha = 0.4
  )
  
  g <- g + xlab("UTM 9N Easting (km)") + ylab("UTM 9N Northing (km)") +
    scale_size(range = c(0, 9)) +
    guides(size = FALSE, fill = FALSE) +
    theme_pbs() +
    coord_equal(
      expand = FALSE, xlim = range(pg_all$X * 10) + map_padding,
      ylim = range(pg_all$Y * 10) + map_padding
    )
  
  g
}


```


```{r}
median(pg_all$pred)
make_plot(pg_all, "pred") + facet_wrap(~ time) +
  scale_fill_gradient2(
    midpoint = median(pg_all$pred),
    low = scales::muted("blue"), 
    mid = "white",
    high = scales::muted("red"))

median(pg_all$sp_field)
make_plot(pg_all, "sp_field") + facet_wrap(~ time) +
  scale_fill_gradient2(
    midpoint = median(pg_all$sp_field),
    high = scales::muted("red"), 
    mid = "white",
    low = scales::muted("blue"))


make_plot(pg_all, "depth_scaled") +
  scale_fill_gradient2(
    midpoint = 0,
    high = scales::muted("blue"), 
    mid = "white",
    low = scales::muted("red"))
```
```


Run using Zuur's polar bear code as template for letting relationship between temp and depth change through time...

```{r}
temp_dat$year_f2 <- temp_dat$year_f
temp_dat$year_f3 <- temp_dat$year_f

f1 <- temperature_C_avg ~ depth_scaled + depth_scaled2 + 
                 f(year_f, model = "rw2") +
                 f(year_f2, depth_scaled, model = "rw2") +
                 f(year_f3, depth_scaled2, model = "rw2") 
  
                               
I1 <- inla(f1, 
           control.predictor = list(compute = TRUE),
           family = "gaussian",
           data = temp_dat
           )
summary(I1)

# We can access the fitted values:
Fit1 <- I1$summary.fitted.values[,"mean"]

# And using these fitted values we can
# calculate Pearson residuals. These can
# be used for model validation purposes.

# Let us plot the 2 trends.
# Here is how we access them:
Yearsm <- I1$summary.random$year_f
Depthre  <- I1$summary.random$year_f2
Depth2re  <- I1$summary.random$year_f3
```

```{r}
# The rest is simple plotting
#par(mfrow = c(1,2), mar = c(5,5,2,2), cex.lab = 1.5)
plot(Yearsm[,1:2], type='l',
     xlab = 'Year', 
     ylab = 'Smoother',
     ylim = c(-0.2, 0.2) )
abline(h=0, lty=3)
lines(Yearsm[, c(1, 4)], lty=2)
lines(Yearsm[, c(1, 6)], lty=2)
text(1988, 0.2, "A", cex = 1.5)


plot(Depthre[,1:2], type='l',
     xlab = 'Depth', 
     ylab = 'Smoother',
     ylim = c(-0.5, 0.6) )
abline(h=0, lty=3)
lines(Depthre[, c(1, 4)], lty=2)
lines(Depthre[, c(1, 6)], lty=2)
text(0, 0.6, "B", cex = 1.5)
```

```{r}
####################################



# Here is a technical problem.
# Hyper-parameters are on different scales, 
# which makes it difficult to find appropriate 
# shape and scale values for each hyper-parameter. 
# Sørbye (2013) recommends applying a scaling 
# so that priors for the precision parameters 
# for certain models (e.g. rw1, rw2, besag, bym) 
# become comparable. This is achieved by either 
# adding scale.model = TRUE to the rw1 and rw2 
# models, or by typing:

inla.setOption(scale.model.default = TRUE)

# We can now specify one set of scale and shape 
# values (instead of different ones for the rw1 and 
# rw2 models), and that should in principle work better.
 

# Let's use the PC prior instead of the gamma prior.
# After pottering around with some values
# for U we tried:

U <- 1.5
hyper.prec = list(theta = list(
                   prior = "pc.prec", 
                   param = c(U, 0.01)))
  
f2 <- Movement ~ fRepro + 
                 f(Year, 
                   model = "rw1",
                   hyper = hyper.prec) +
                 f(DayInYear, 
                   model = "rw2",
                   hyper = hyper.prec
                   ) +
                 f(BearN, model = "iid")  
                               
I2 <- inla(f2, 
           control.predictor = list(compute = TRUE),
           family = "gamma",
           data = PB
           )
summary(I2)
Fit2 <- I2$summary.fitted.values[,"mean"]


# Access the trends
Yearsm <- I2$summary.random$Year
Daysm  <- I2$summary.random$DayInYear

# And plot the trends
par(mfrow = c(1,2), mar = c(5,5,2,2), cex.lab = 1.5)
Time <- Yearsm[,1] #* sd(PB$Year) + mean(PB$Year)
plot(x = Time,
     y = Yearsm[,2], type='l',
     xlab = 'Year', 
     ylab = 'Trend',
     ylim = c(-0.5, 0.5) )
abline(h=0, lty=3)
lines(x = Time, y = Yearsm[, c(4)], lty=2)
lines(x = Time, y = Yearsm[, c(6)], lty=2)
text(1988, 0.5, "A", cex = 1.5)


Time <- Daysm[,1] #* sd(PB$DayInYear) + mean(PB$DayInYear)
plot(x = Time,
     y = Daysm[,2], type='l',
     xlab = 'DayInYear', 
     ylab = 'Trend',
     ylim = c(-0.5, 0.5) )
abline(h=0, lty=3)
lines(Time, Daysm[, c(4)], lty=2)
lines(Time, Daysm[, c(6)], lty=2)
text(0, 0.5, "B", cex = 1.5)


# Nice and smooth.



# For your information: The rw1 and rw2 are
# also your smoother options (in case you want
# to do GAM). Time allowing we will discuss
# better options.


```




Sean's code that I dont' remember where i found...


```{r}

####
# names(d) <- tolower(names(d))
# d$species_common_name <- tolower(d$species_common_name)
# d$species_science_name <- tolower(d$species_science_name)
# d$year <- lubridate::year(d$trip_start_date)
# 
# sp <- filter(d, species_common_name %in% "pacific ocean perch") %>% 
#   filter(!is.na(catch_weight)) %>% 
#   filter(year %in% seq(2004, 2012, 2))
# 
# dat <- sp
# dat <- filter(dat, start_lon > -128, start_lon < 125, start_lat > 48, start_lat < 50.3)
# nrow(dat)
# 
# dat <- filter(dat, fe_bottom_water_temperature > 1, fe_bottom_water_temperature < 12,
#   !is.na(fe_bottom_water_temp_depth), !is.na(fe_bottom_water_temperature))
# nrow(dat)
# 
# dat <- select(dat, year, start_lon, start_lat, catch_weight, fe_bottom_water_temp_depth) %>% 
#   rename(X = start_lon, Y = start_lat)

dat <- temp_dat

attr(dat, "projection") <- "LL"


dat <- PBSmapping::convUL(dat)
dat <- as.data.frame(na.omit(dat))

subcoords = cbind(dat$X, dat$Y)

bnd = inla.nonconvex.hull(subcoords, convex = 30)
mesh1 = inla.mesh.2d(
  boundary = bnd,
  max.edge = c(15, 40),
  cutoff = 5,
  offset = 1
)
plot(mesh1)
points(dat$X, dat$Y, col = "red")
summary(mesh1)



##########
A.data <- inla.spde.make.A(mesh1, loc = cbind(dat$X, dat$Y))

# Make SPDE based on mesh
spde = inla.spde2.matern(mesh1, alpha = 3 / 2)

n = nrow(dat)
YEARS <- unique(dat$year)
k = length(unique(dat$year))

# Make a design matrix where the first year is the intercept
# ...
dm <- matrix(nrow = nrow(dat), ncol = 2)
dm[,1] <- 1

#########

dm[,2] <- dat$fe_bottom_water_temp_depth

########

iset <- inla.spde.make.index("i2D", n.spde = mesh1$n, n.group = k)

X.1 = dm
Covar.names <- c("intercept", "depth")
XX.list <- as.list(X.1)
effect.list <- list()
#   effect.list[[1]] <- c(iset, list(Intercept=1))
effect.list[[1]] <- c(iset)
for (Z in 1:ncol(X.1))
  effect.list[[Z + 1]] <- XX.list[[Z]]
names(effect.list) <- c("1", Covar.names)

### Make data stack.
A <-
  inla.spde.make.A(
    mesh = mesh1,
    loc = cbind(dat$X, dat$Y),
    group = dat$year
  )
A.list = list()
A.list[[1]] = A
for (Z in 1:ncol(X.1))
  A.list[[Z + 1]] <- 1

### Make projection points stack.
Ntrials <- rep(1, length(dat$Y))

sdat <-
  inla.stack(
    tag = 'stdata',
    data = list(
      y = dat$Y,
      link = 1,
      Ntrials = Ntrials
    ),
    A = A.list,
    effects = effect.list
  )

formula = as.formula(
  paste0(
    "y ~ -1 +",
    paste(Covar.names, collapse = "+"),
    "+ f(i2D, model=spde, group = i2D.group, control.group = list(model='ar1'))"
  )
)		# field evolves with AR1 by year

inlaModel <-
  inla(
    formula,
    family = "gamma",
    data = inla.stack.data(sdat),
    control.predictor = list(compute = TRUE, A = inla.stack.A(sdat)),
    verbose = TRUE,
    debug = TRUE,
    keep = FALSE,
    control.compute = list(dic = TRUE, cpo = TRUE, config = TRUE),
    control.fixed = list(correlation.matrix = TRUE),
    control.inla = list(lincomb.derived.correlation.matrix = TRUE)
  )
```

