---
title: "R Notebook"
output: html_notebook
---

```{r setup}
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) 
library(gfplot)
library(lattice)
library(INLA)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(mapdata)
library(lubridate)
```

```{r}
ssids <- get_ssids()
ssids
```

```{r}
raw <- readRDS("data-cache/pacific-cod.rds")
#dat$survey_abbrev
glimpse(raw)
```

```{r}
testdat <- tidy_survey_sets(raw, c("SYN HS","SYN QCS","SYN WCVI","SYN WCHG"), years = c(2015, 2016, 2017, 2018))
#testdat <- gfplot:::interp_survey_bathymetry(testdat)
glimpse(testdat)
View(testdat[[1]])
```

```{r}
# get_ssids()
sensor_syn <- get_sensor_data_trawl(ssid = c(1, 3, 4, 16), spread_attributes = TRUE)
# View(sensor_syn)
saveRDS(sensor_syn, "data-cache/allsynoptic_sensor_07mar19_time2.rds")
# write.csv(sensor, "synoptic_3_sensor_data_07mar19.csv")
# sensor <- read_csv("data-cache/synoptic_3_sensor_data_07mar19.csv", col_types = cols(do_mlpL_avg = col_double(), 
#                     do_mlpL_max = col_double(), do_mlpL_min = col_double(), salinity_PSU_avg = col_double(), salinity_PSU_max = col_double(), 
#                     salinity_PSU_min = col_double()), na = "NA")
sensor <- readRDS("data-cache/allsynoptic_sensor_07mar19_time2.rds")
View(sensor)
```


```{r}
dat <- left_join(testdat[[1]], sensor, by="fishing_event_id", suffix = c("", ".sensor"))
dat$year_f <- factor(dat$year)

View(dat)

dat <- dat %>% mutate(do_range = do_mlpL_max - do_mlpL_min, salinity_range = salinity_PSU_max -
  salinity_PSU_min, temp_range = temperature_C_max - temperature_C_min, depth_range = depth_m_max - depth_m_min)
glimpse(dat)
```

```{r}
plot(depth ~ depth_m_avg, data = dat) # both depth variables seem to agree

plot(temp_range ~ depth_range, data = dat) 
plot(do_range ~ depth_range, data = dat) 
plot(salinity_range ~ depth_range, data = dat) 

plot(temperature_C_avg ~ temp_range, data = dat) #variance in temp estimate is highest for middle values
plot(do_mlpL_avg ~ do_range, data = dat) # one outlier at range = 8, others < 4
plot(do_range ~ depth, data = dat) # but outlier is at middle depth
plot(salinity_PSU_avg ~ salinity_range, data = dat) # 5 extreme values range > 10 and abnormally low mean salinity 
plot(salinity_PSU_avg ~ depth, cex = salinity_range/3, ylim=c(25,35), data = dat)
plot(salinity_PSU_max ~ depth, cex = salinity_range/3, ylim=c(25,35), data = dat)
plot(do_mlpL_avg ~ depth, cex = do_range, data = dat) 
plot(temperature_C_avg ~ depth, cex = temp_range, data = dat)

plot(do_mlpL_avg ~ temperature_C_avg, cex = depth/100, data = dat)
plot(do_mlpL_avg ~ salinity_PSU_avg, cex = depth/100, data = dat)

plot(-log(depth) ~ do_mlpL_avg,  cex = depth_range/35, data = dat) 
plot(-log(depth) ~ temperature_C_avg,  cex = depth_range/35, data = dat)
plot(-log(depth) ~ salinity_PSU_max,  cex = depth_range/35, data = dat)


```

```{r}
dat2 <- dat %>% filter(temperature_C_avg!="NA")

ggplot(dat2, aes(x=depth, y=temperature_C_avg)) + 
  geom_point(alpha=0.25) + 
  geom_point(data=subset(dat2, temp_range>1), alpha=0.5) + 
  geom_errorbar(data=subset(dat2, temp_range>1),aes(ymin=temperature_C_min, ymax=temperature_C_max),alpha= 0.75, colour="blue") + 
  geom_errorbarh(data=subset(dat2, temp_range>1),aes(xmin=depth_m_min, xmax=depth_m_max),alpha= 0.75, colour="red") + 
  ylim(2,12) + 
  facet_wrap(~survey_desc, scales = "free")
```

```{r}
dat2 <- dat %>% filter(do_mlpL_avg!="NA")

ggplot(dat2, aes(x=depth, y=do_mlpL_avg)) + 
  geom_point(alpha=0.25) + 
  geom_point(data=subset(dat2, do_range>1), alpha=0.5) + 
  geom_errorbar(data=subset(dat2, do_range>1), aes(ymin=do_mlpL_min, ymax=do_mlpL_max), alpha= 0.75, colour="blue") + 
  geom_errorbarh(data=subset(dat2, do_range>1), aes(xmin=depth_m_min, xmax=depth_m_max), alpha= 0.75, colour="red") + 
  ylim(0,10) + 
  facet_wrap(~survey_desc, scales = "free")
```

```{r}
dat2 <- dat %>% filter(salinity_PSU_avg!="NA")

ggplot(dat2, aes(x=depth, y=salinity_PSU_avg)) + 
  geom_point(alpha=0.25) + 
  geom_point(data=subset(dat2, salinity_range>1), alpha=0.5) + 
  geom_errorbar(data=subset(dat2, salinity_range>1), aes(ymin=salinity_PSU_min, ymax=salinity_PSU_max), alpha= 0.75, colour="blue") + 
  geom_errorbarh(data=subset(dat2, salinity_range>1), aes(xmin=depth_m_min, xmax=depth_m_max), alpha= 0.75, colour="red") + 
  #ylim(0,10) + 
  facet_wrap(~survey_desc, scales = "free")
```


```{r}
out <- dat %>% filter(depth_range>80)
out

pair <- dat %>% filter(ssid=="1") %>% select(., depth_range, temp_range, do_range, salinity_range, depth_m_avg,do_mlpL_avg,temperature_C_avg,salinity_PSU_max)
pairs(pair)  
```

Scale predictors by modifying code from gfplot to include environmental variables...
Changes should be added to gfplot?
```{r}
scale_survey_predictors <- function(dat) {
  if (sum(is.na(dat$depth)) > 0) {
    dat$depth[is.na(dat$depth)] <- dat$akima_depth[is.na(dat$depth)]
  }
  mutate(dat,
    # depth
    depth_mean = mean(log(depth), na.rm = TRUE),
    depth_c = (log(depth) - depth_mean[1]),
    depth_sd = sd(log(depth), na.rm = TRUE),
    depth_scaled = (log(depth) - depth_mean[1]) / depth_sd[1],
    depth_scaled2 = depth_scaled^2,
    # temperature
    temp_mean = mean(log(temperature_C_avg), na.rm = TRUE),
    temp_c = (log(temperature_C_avg) - temp_mean[1]),
    temp_sd = sd(log(temperature_C_avg), na.rm = TRUE),
    temp_scaled = (log(temperature_C_avg) - temp_mean[1]) / temp_sd[1],
    temp_scaled2 = temp_scaled^2,
    # dissolved O2
    do_mean = mean(log(do_mlpL_avg), na.rm = TRUE),
    do_c = (log(do_mlpL_avg) - do_mean[1]),
    do_sd = sd(log(do_mlpL_avg), na.rm = TRUE),
    do_scaled = (log(do_mlpL_avg) - do_mean[1]) / do_sd[1],
    do_scaled2 = do_scaled^2,
    # salinity
    salinity_mean = mean(log(salinity_PSU_avg), na.rm = TRUE),
    salinity_c = (log(salinity_PSU_avg) - salinity_mean[1]),
    salinity_sd = sd(log(salinity_PSU_avg), na.rm = TRUE),
    salinity_scaled = (log(salinity_PSU_avg) - salinity_mean[1]) / salinity_sd[1],
    salinity_scaled2 = salinity_scaled^2,
    # to put spatial decay parameter on right scale
    X10 = X / 10, Y10 = Y / 10 
  )
}
```

```{r}
scaled <- scale_survey_predictors(dat)
# glimpse(scaled)
synhs <- scaled %>% filter(do_mlpL_avg!="NA") %>% filter (density!="NA") 
glimpse(synhs)
```


```{r}
ggplot(synhs, aes(depth)) + geom_histogram() 
ggplot(synhs, aes(density*1000)) + geom_histogram() 
ggplot(synhs, aes(depth, (density*1000))) + geom_point() 
#ggplot(synhs, aes(depth_m, log(density*1000+1))) + geom_point() 
```

######################################
# Start Bayesian analysis
# One more time the model:

# Movement_ij      ~ Gamma(mu_ik, r)
# E(Movement_ij)   = mu_ij
# var(Movement_ij) = mu_ij^2 / r 
# mu_ij = exp(Intercept + fRepro + Annual Trend + Seasonal Trend + a_i)


# Below we define a model with a factor fRepro, a long term
# trend for Year, and seasonal trend for DayInYear, and
# a random intercept for bear.  

# Each trend is modelled as a random walk trend.
# So each of them has a sigma.

```{r}


####
names(d) <- tolower(names(d))
d$species_common_name <- tolower(d$species_common_name)
d$species_science_name <- tolower(d$species_science_name)
d$year <- lubridate::year(d$trip_start_date)

sp <- filter(d, species_common_name %in% "pacific ocean perch") %>% 
  filter(!is.na(catch_weight)) %>% 
  filter(year %in% seq(2004, 2012, 2))

dat <- sp
dat <- filter(dat, start_lon > -128, start_lon < 125, start_lat > 48, start_lat < 50.3)
nrow(dat)

dat <- filter(dat, fe_bottom_water_temperature > 1, fe_bottom_water_temperature < 12,
  !is.na(fe_bottom_water_temp_depth), !is.na(fe_bottom_water_temperature))
nrow(dat)

dat <- select(dat, year, start_lon, start_lat, catch_weight, fe_bottom_water_temp_depth) %>% 
  rename(X = start_lon, Y = start_lat)

attr(dat, "projection") <- "LL"


dat <- PBSmapping::convUL(dat)
dat <- as.data.frame(na.omit(dat))

subcoords = cbind(dat$X, dat$Y)

bnd = inla.nonconvex.hull(subcoords, convex = 30)
mesh1 = inla.mesh.2d(
  boundary = bnd,
  max.edge = c(15, 40),
  cutoff = 5,
  offset = 1
)
plot(mesh1)
points(dat$X, dat$Y, col = "red")
summary(mesh1)



##########
A.data <- inla.spde.make.A(mesh1, loc = cbind(dat$X, dat$Y))

# Make SPDE based on mesh
spde = inla.spde2.matern(mesh1, alpha = 3 / 2)

n = nrow(dat)
YEARS <- unique(dat$year)
k = length(unique(dat$year))

# Make a design matrix where the first year is the intercept
# ...
dm <- matrix(nrow = nrow(dat), ncol = 2)
dm[,1] <- 1
dm[,2] <- dat$fe_bottom_water_temp_depth

iset <- inla.spde.make.index("i2D", n.spde = mesh1$n, n.group = k)

X.1 = dm
Covar.names <- c("intercept", "depth")
XX.list <- as.list(X.1)
effect.list <- list()
#   effect.list[[1]] <- c(iset, list(Intercept=1))
effect.list[[1]] <- c(iset)
for (Z in 1:ncol(X.1))
  effect.list[[Z + 1]] <- XX.list[[Z]]
names(effect.list) <- c("1", Covar.names)

### Make data stack.
A <-
  inla.spde.make.A(
    mesh = mesh1,
    loc = cbind(dat$X, dat$Y),
    group = dat$year
  )
A.list = list()
A.list[[1]] = A
for (Z in 1:ncol(X.1))
  A.list[[Z + 1]] <- 1

### Make projection points stack.
Ntrials <- rep(1, length(dat$Y))

sdat <-
  inla.stack(
    tag = 'stdata',
    data = list(
      y = dat$Y,
      link = 1,
      Ntrials = Ntrials
    ),
    A = A.list,
    effects = effect.list
  )

formula = as.formula(
  paste0(
    "y ~ -1 +",
    paste(Covar.names, collapse = "+"),
    "+ f(i2D, model=spde, group = i2D.group, control.group = list(model='ar1'))"
  )
)		# field evolves with AR1 by year

inlaModel <-
  inla(
    formula,
    family = "gamma",
    data = inla.stack.data(sdat),
    control.predictor = list(compute = TRUE, A = inla.stack.A(sdat)),
    verbose = TRUE,
    debug = TRUE,
    keep = FALSE,
    control.compute = list(dic = TRUE, cpo = TRUE, config = TRUE),
    control.fixed = list(correlation.matrix = TRUE),
    control.inla = list(lincomb.derived.correlation.matrix = TRUE)
  )
```






```{r}

synhs$year_f2 <- synhs$year_f
synhs$year_f3 <- synhs$year_f


f1 <- temperature_C2 ~ depth_scaled + depth_scaled2 + 
                 f(year_f, model = "rw2") +
                 f(year_f2, depth_m, model = "rw2") +
                 f(year_f3, depth2, model = "rw2") 
  
                               
I1 <- inla(f1, 
           control.predictor = list(compute = TRUE),
           family = "gamma",
           data = synhs
           )
summary(I1)

# We can access the fitted values:
Fit1 <- I1$summary.fitted.values[,"mean"]

# And using these fitted values we can
# calculate Pearson residuals. These can
# be used for model validation purposes.

# Let us plot the 2 trends.
# Here is how we access them:
Yearsm <- I1$summary.random$Year
Daysm  <- I1$summary.random$DayInYear


# The rest is simple plotting
#par(mfrow = c(1,2), mar = c(5,5,2,2), cex.lab = 1.5)
plot(Yearsm[,1:2], type='l',
     xlab = 'Year', 
     ylab = 'Smoother',
     ylim = c(-0.2, 0.2) )
abline(h=0, lty=3)
lines(Yearsm[, c(1, 4)], lty=2)
lines(Yearsm[, c(1, 6)], lty=2)
text(1988, 0.2, "A", cex = 1.5)


plot(Daysm[,1:2], type='l',
     xlab = 'DayInYear', 
     ylab = 'Smoother',
     ylim = c(-0.5, 0.6) )
abline(h=0, lty=3)
lines(Daysm[, c(1, 4)], lty=2)
lines(Daysm[, c(1, 6)], lty=2)
text(0, 0.6, "B", cex = 1.5)

####################################



# Here is a technical problem.
# Hyper-parameters are on different scales, 
# which makes it difficult to find appropriate 
# shape and scale values for each hyper-parameter. 
# SÃ¸rbye (2013) recommends applying a scaling 
# so that priors for the precision parameters 
# for certain models (e.g. rw1, rw2, besag, bym) 
# become comparable. This is achieved by either 
# adding scale.model = TRUE to the rw1 and rw2 
# models, or by typing:

inla.setOption(scale.model.default = TRUE)

# We can now specify one set of scale and shape 
# values (instead of different ones for the rw1 and 
# rw2 models), and that should in principle work better.
 

# Let's use the PC prior instead of the gamma prior.
# After pottering around with some values
# for U we tried:

U <- 1.5
hyper.prec = list(theta = list(
                   prior = "pc.prec", 
                   param = c(U, 0.01)))
  
f2 <- Movement ~ fRepro + 
                 f(Year, 
                   model = "rw1",
                   hyper = hyper.prec) +
                 f(DayInYear, 
                   model = "rw2",
                   hyper = hyper.prec
                   ) +
                 f(BearN, model = "iid")  
                               
I2 <- inla(f2, 
           control.predictor = list(compute = TRUE),
           family = "gamma",
           data = PB
           )
summary(I2)
Fit2 <- I2$summary.fitted.values[,"mean"]


# Access the trends
Yearsm <- I2$summary.random$Year
Daysm  <- I2$summary.random$DayInYear

# And plot the trends
par(mfrow = c(1,2), mar = c(5,5,2,2), cex.lab = 1.5)
Time <- Yearsm[,1] #* sd(PB$Year) + mean(PB$Year)
plot(x = Time,
     y = Yearsm[,2], type='l',
     xlab = 'Year', 
     ylab = 'Trend',
     ylim = c(-0.5, 0.5) )
abline(h=0, lty=3)
lines(x = Time, y = Yearsm[, c(4)], lty=2)
lines(x = Time, y = Yearsm[, c(6)], lty=2)
text(1988, 0.5, "A", cex = 1.5)


Time <- Daysm[,1] #* sd(PB$DayInYear) + mean(PB$DayInYear)
plot(x = Time,
     y = Daysm[,2], type='l',
     xlab = 'DayInYear', 
     ylab = 'Trend',
     ylim = c(-0.5, 0.5) )
abline(h=0, lty=3)
lines(Time, Daysm[, c(4)], lty=2)
lines(Time, Daysm[, c(6)], lty=2)
text(0, 0.5, "B", cex = 1.5)


# Nice and smooth.



# For your information: The rw1 and rw2 are
# also your smoother options (in case you want
# to do GAM). Time allowing we will discuss
# better options.



###################
# TRY WITH interaction
inla.list.models()
# te = two-dimensional smoother

library(dplyr)

PB$Year2 <- PB$Year
PB$DayInYear2 <- PB$DayInYear



PB2 <- PB %>% filter(Year > 1991)


U <- 1.5

hyper.prec = list(theta = list(
  prior = "pc.prec", 
  param = c(U, 0.05)))

  

f3 <- Movement ~ fRepro + 
  f(Year, 
    model = "rw2",
    hyper = hyper.prec) +
  f(DayInYear, Year2,
    model = "rw2",
    hyper = hyper.prec) +  
  f(BearN, model = "iid")  

I3 <- inla(f3, 
           control.predictor = list(compute = TRUE),
           family = "gamma",
           data = PB2
)
summary(I3)
Fit3 <- I3$summary.fitted.values[,"mean"]


# Access the trends
Yearsm <- I3$summary.random$Year
Daysm  <- I3$summary.random$DayInYear

# And plot the trends
#par(mfrow = c(1,2), mar = c(5,5,2,2), cex.lab = 1.5)
Time <- Yearsm[,1] #* sd(PB$Year) + mean(PB$Year)
plot(x = Time,
     y = Yearsm[,2], type='l',
     xlab = 'Year', 
     ylab = 'Trend',
     ylim = c(-0.5, 0.5) )
abline(h=0, lty=3)
lines(x = Time, y = Yearsm[, c(4)], lty=2)
lines(x = Time, y = Yearsm[, c(6)], lty=2)
text(1988, 0.5, "A", cex = 1.5)


Time <- Daysm[,1] #* sd(PB$DayInYear) + mean(PB$DayInYear)
plot(x = Time,
     y = Daysm[,2], type='l',
     xlab = 'DayInYear', 
     ylab = 'Trend',
     ylim = c(-0.5, 0.5) )
abline(h=0, lty=3)
lines(Time, Daysm[, c(4)], lty=2)
lines(Time, Daysm[, c(6)], lty=2)
text(0, 0.5, "B", cex = 1.5)

```



